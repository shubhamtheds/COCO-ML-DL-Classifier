{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HY_5n2Di6cMf8u0LxKFWck705vbEfPmM",
      "authorship_tag": "ABX9TyNEoNfiNptssuiNYYREO9HT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamtheds/COCO-ML-DL-Classifier/blob/main/coco_multi_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import random\n",
        "import hashlib\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "import joblib"
      ],
      "metadata": {
        "id": "QqUWH96X2eny"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "0EPIrDaeuiwm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('requirements.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "p-q9cRkguylF",
        "outputId": "469b7390-7ded-4704-a6aa-43b7f171627e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2234eb81-48ef-47bf-afa6-9b4484bc04e3\", \"requirements.txt\", 7862)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDatasetDownloader:\n",
        "    def __init__(self, object_classes, num_images_per_class, min_object_size, image_size):\n",
        "        self.dataset_url = 'http://images.cocodataset.org/zips/train2017.zip'\n",
        "        self.annotations_url = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "        self.dataset_folder = '/content/coco_dataset'\n",
        "        self.resized_folder = '/content/coco_resized'\n",
        "        self.annotations_folder = '/content'\n",
        "        self.object_classes = object_classes\n",
        "        self.num_images_per_class = num_images_per_class\n",
        "        self.min_object_size = min_object_size\n",
        "        self.image_size = image_size\n",
        "    \n",
        "    def download_dataset(self):\n",
        "        # Downloading COCO dataset\n",
        "        if not os.path.exists(self.dataset_folder):\n",
        "            print('Downloading COCO dataset...')\n",
        "            response = requests.get(self.dataset_url, stream=True)\n",
        "            with open('train2017.zip', 'wb') as f:\n",
        "                shutil.copyfileobj(response.raw, f)\n",
        "            with zipfile.ZipFile('train2017.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.dataset_folder)\n",
        "            os.remove('train2017.zip')\n",
        "\n",
        "        # Downloading COCO annotations\n",
        "        if not os.path.exists(os.path.join(self.annotations_folder, 'annotations')):\n",
        "            print('Downloading COCO annotations...')\n",
        "            response = requests.get(self.annotations_url, stream=True)\n",
        "            with open('annotations_trainval2017.zip', 'wb') as f:\n",
        "                shutil.copyfileobj(response.raw, f)\n",
        "            with zipfile.ZipFile('annotations_trainval2017.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.annotations_folder)\n",
        "            os.remove('annotations_trainval2017.zip')\n",
        "\n",
        "    def process_dataset(self):\n",
        "        coco = COCO(os.path.join(self.annotations_folder, 'annotations', 'instances_train2017.json'))\n",
        "\n",
        "        categories = coco.loadCats(coco.getCatIds())\n",
        "        category_map = {category['id']: category['name'] for category in categories}\n",
        "\n",
        "        # Initializing dictionaries to store images and counts per class\n",
        "        class_images = {object_class: [] for object_class in self.object_classes}\n",
        "        class_counts = {object_class: 0 for object_class in self.object_classes}\n",
        "\n",
        "        for image_id in coco.getImgIds():\n",
        "            image = coco.loadImgs(image_id)[0]\n",
        "            annotations = coco.loadAnns(coco.getAnnIds(imgIds=image['id']))\n",
        "\n",
        "            # Check if image contains desired object\n",
        "            object_class_present = False\n",
        "            for annotation in annotations:\n",
        "                category_id = annotation['category_id']\n",
        "                if category_map[category_id] in self.object_classes:\n",
        "                    object_class_present = True\n",
        "                    break\n",
        "\n",
        "            if not object_class_present:\n",
        "                continue\n",
        "\n",
        "            image_path = os.path.join(self.dataset_folder, 'train2017', image['file_name'])\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            for annotation in annotations:\n",
        "                category_id = annotation['category_id']\n",
        "                object_class = category_map[category_id]\n",
        "\n",
        "                if object_class not in self.object_classes:\n",
        "                    continue\n",
        "\n",
        "                # Checking if size of object is big enough\n",
        "                bbox = annotation['bbox']\n",
        "                object_size = max(bbox[2], bbox[3])\n",
        "                if object_size < self.min_object_size:\n",
        "                    continue\n",
        "\n",
        "                # Check if we have already collected enough images for this class\n",
        "                if class_counts[object_class] >= self.num_images_per_class:\n",
        "                    continue\n",
        "\n",
        "                # Croping and resizing image\n",
        "                x, y, w, h = [int(val) for val in bbox]\n",
        "                x1, y1, x2, y2 = x, y, x + w, y + h\n",
        "                crop_size = max(w, h)\n",
        "                center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "                x1 = center_x - crop_size // 2\n",
        "                y1 = center_y - crop_size // 2\n",
        "                x2 = x1 + crop_size\n",
        "                y2 = y1 + crop_size\n",
        "                crop = img[y1:y2, x1:x2]\n",
        "\n",
        "                if crop.size == 0:\n",
        "                    continue\n",
        "                resized_crop = cv2.resize(crop, (self.image_size, self.image_size))\n",
        "\n",
        "                # Compute hash of image and check if it has already been saved\n",
        "                with open(image_path, 'rb') as f:\n",
        "                    image_data = f.read()\n",
        "                image_hash = hashlib.sha256(image_data).hexdigest()\n",
        "                if image_hash not in class_images[object_class]:\n",
        "                    # Saving image to new path\n",
        "                    folder_path = os.path.join(self.resized_folder, object_class)\n",
        "                    if not os.path.exists(folder_path):\n",
        "                        os.makedirs(folder_path)\n",
        "                    image_path = os.path.join(folder_path, f'{image_hash}.jpg')\n",
        "                    cv2.imwrite(image_path, resized_crop)\n",
        "    \n",
        "                    class_images[object_class].append(image_hash)\n",
        "                    class_counts[object_class] += 1\n",
        "    \n",
        "                # exit loop after collecting images\n",
        "                if class_counts[object_class] >= self.num_images_per_class:\n",
        "                    break"
      ],
      "metadata": {
        "id": "eWsB1WgTy1eZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "image_size = 32\n",
        "num_images_per_class = 5000\n",
        "min_object_size = 16\n",
        "\n",
        "downloader = CocoDatasetDownloader(object_classes, num_images_per_class, min_object_size, image_size)\n",
        "downloader.download_dataset()\n",
        "downloader.process_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDvMSUcv0E9j",
        "outputId": "72797208-5869-4798-db6a-ecc5972b2b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading COCO dataset...\n",
            "Downloading COCO annotations...\n",
            "loading annotations into memory...\n",
            "Done (t=26.73s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLASSICAL ML APPROACH"
      ],
      "metadata": {
        "id": "ZQN-qXgGFFVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassifier:\n",
        "    def __init__(self, dataset_folder, object_classes, test_size=0.2, random_state=42, stratify=None):\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.object_classes = object_classes\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.stratify = stratify\n",
        "    \n",
        "    def load_data(self):\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for object_class in self.object_classes:\n",
        "            folder_path = os.path.join(self.dataset_folder, object_class)\n",
        "            image_paths = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
        "            for image_path in image_paths:\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is not None:\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
        "                    image = cv2.resize(image, (32, 32))\n",
        "                    X.append(image)\n",
        "                    y.append(object_class)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Spliting data into train and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=self.stratify)\n",
        "\n",
        "        # Reshaping images to 2D arrays\n",
        "        nsamples, nx, ny, nrgb = X_train.shape\n",
        "        X_train = X_train.reshape((nsamples, nx * ny * nrgb))\n",
        "        nsamples, nx, ny, nrgb = X_val.shape\n",
        "        X_val = X_val.reshape((nsamples, nx * ny * nrgb))\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.X_val = X_val\n",
        "        self.y_train = y_train\n",
        "        self.y_val = y_val\n",
        "    \n",
        "    def train_and_evaluate(self, classifiers):\n",
        "        for clf_dict in classifiers:\n",
        "            clf_name = clf_dict['name']\n",
        "            clf = clf_dict['clf'](**clf_dict['params'])\n",
        "            clf.fit(self.X_train, self.y_train)\n",
        "            y_val_pred = clf.predict(self.X_val)\n",
        "            print(f\"{clf_name} Classification Report:\")\n",
        "            print(classification_report(self.y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "iJCKd1tX4tOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = '/content/coco_resized'\n",
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "classifiers = [{'name': 'Random Forest', 'clf': RandomForestClassifier, 'params': {'n_estimators': 200}}]\n",
        "\n",
        "ic = ImageClassifier(dataset_folder, object_classes)\n",
        "ic.load_data()\n",
        "ic.train_and_evaluate(classifiers)"
      ],
      "metadata": {
        "id": "njSqOn9C4u4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEEP LEARNING APPROACH WITH KERAS API\n"
      ],
      "metadata": {
        "id": "_EzaVSpy1F6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectClassifier:\n",
        "    def __init__(self, dataset_folder, object_classes):\n",
        "      # Initializing the ObjectClassifier class with the dataset folder and object classes\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.object_classes = object_classes\n",
        "        self.num_classes = len(self.object_classes)\n",
        "        self.label_map = {object_class: i for i, object_class in enumerate(self.object_classes)}\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "      # Loading and preprocessing the data\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for object_class in self.object_classes:\n",
        "            folder_path = os.path.join(self.dataset_folder, object_class)\n",
        "            image_paths = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
        "            for image_path in image_paths:\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is not None:\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    image = cv2.resize(image, (32, 32))\n",
        "                    X.append(image)\n",
        "                    y.append(object_class)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Splitting the data into training and validation sets and normalizing the pixel values\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        X_train = X_train / 255.0\n",
        "        X_val = X_val / 255.0\n",
        "\n",
        "        # Converting the labels to numerical values and one-hot encoding them\n",
        "        y_train_num = np.array([self.label_map[label] for label in y_train])\n",
        "        y_val_num = np.array([self.label_map[label] for label in y_val])\n",
        "\n",
        "        y_train_onehot = to_categorical(y_train_num)\n",
        "        y_val_onehot = to_categorical(y_val_num)\n",
        "\n",
        "        return X_train, X_val, y_train_onehot, y_val_onehot, y_val_num\n",
        "\n",
        "    def create_model(self):\n",
        "        # Creating a convolutional neural network model\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model  \n",
        "\n",
        "    def train_and_evaluate(self, num_epochs=100, batch_size=64, patience=100):\n",
        "      # Training and evaluating the model\n",
        "        X_train, X_val, y_train_onehot, y_val_onehot, y_val_num = self.load_and_preprocess_data()\n",
        "        model = self.create_model()\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss', mode='max', patience=patience)\n",
        "\n",
        "        history = model.fit(X_train, y_train_onehot, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val_onehot), callbacks=[early_stop])\n",
        "\n",
        "        y_pred_onehot = model.predict(X_val)\n",
        "        y_pred_num = np.argmax(y_pred_onehot, axis=1)\n",
        "\n",
        "        # Generating the classification report and printing it\n",
        "        report = classification_report(y_val_num, y_pred_num, target_names=self.object_classes)\n",
        "        print('\\nClassification Report:')\n",
        "        print(report)\n",
        "\n",
        "        accuracy = accuracy_score(y_val_num, y_pred_num)\n",
        "\n",
        "        # Ploting average of F1 score, accuracy, precision, and recall\n",
        "        report_dict = classification_report(y_val_num, y_pred_num, target_names=self.object_classes, output_dict=True)\n",
        "        averages = ['macro avg', 'weighted avg']\n",
        "        plot_data = {}\n",
        "\n",
        "        for avg in averages:\n",
        "            plot_data[avg] = [\n",
        "                report_dict[avg]['precision'],\n",
        "                report_dict[avg]['recall'],\n",
        "                report_dict[avg]['f1-score'],\n",
        "                accuracy\n",
        "            ]\n",
        "\n",
        "        plot_df = pd.DataFrame.from_dict(plot_data, orient='index', columns=['precision', 'recall', 'F1-score', 'accuracy'])\n",
        "        sns.set(style=\"whitegrid\")\n",
        "        sns.set_palette(\"pastel\")\n",
        "        ax = sns.barplot(data=plot_df,  palette=sns.color_palette(\"pastel\"), linewidth=1.5)\n",
        "        ax.set(ylabel='Metrics', xlabel='Average', title='Average of F1 score, accuracy, precision and recall')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OMV6p7F_EfSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "dataset_folder = '/content/coco_resized'\n",
        "\n",
        "classifier = ObjectClassifier(dataset_folder, object_classes)\n",
        "classifier.train_and_evaluate()"
      ],
      "metadata": {
        "id": "8ZnA7iZNEfWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kCLwF1RUObY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}