{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HY_5n2Di6cMf8u0LxKFWck705vbEfPmM",
      "authorship_tag": "ABX9TyNEoNfiNptssuiNYYREO9HT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhamtheds/COCO-ML-DL-Classifier/blob/main/coco_multi_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "import random\n",
        "import hashlib\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "import joblib"
      ],
      "metadata": {
        "id": "QqUWH96X2eny"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "0EPIrDaeuiwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('requirements.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "p-q9cRkguylF",
        "outputId": "469b7390-7ded-4704-a6aa-43b7f171627e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2234eb81-48ef-47bf-afa6-9b4484bc04e3\", \"requirements.txt\", 7862)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDatasetDownloader:\n",
        "    def __init__(self, object_classes, num_images_per_class, min_object_size, image_size):\n",
        "        self.dataset_url = 'http://images.cocodataset.org/zips/train2017.zip'\n",
        "        self.annotations_url = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip'\n",
        "        self.dataset_folder = '/content/coco_dataset'\n",
        "        self.resized_folder = '/content/coco_resized'\n",
        "        self.annotations_folder = '/content'\n",
        "        self.object_classes = object_classes\n",
        "        self.num_images_per_class = num_images_per_class\n",
        "        self.min_object_size = min_object_size\n",
        "        self.image_size = image_size\n",
        "    \n",
        "    def download_dataset(self):\n",
        "        # Downloading COCO dataset\n",
        "        if not os.path.exists(self.dataset_folder):\n",
        "            print('Downloading COCO dataset...')\n",
        "            response = requests.get(self.dataset_url, stream=True)\n",
        "            with open('train2017.zip', 'wb') as f:\n",
        "                shutil.copyfileobj(response.raw, f)\n",
        "            with zipfile.ZipFile('train2017.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.dataset_folder)\n",
        "            os.remove('train2017.zip')\n",
        "\n",
        "        # Downloading COCO annotations\n",
        "        if not os.path.exists(os.path.join(self.annotations_folder, 'annotations')):\n",
        "            print('Downloading COCO annotations...')\n",
        "            response = requests.get(self.annotations_url, stream=True)\n",
        "            with open('annotations_trainval2017.zip', 'wb') as f:\n",
        "                shutil.copyfileobj(response.raw, f)\n",
        "            with zipfile.ZipFile('annotations_trainval2017.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.annotations_folder)\n",
        "            os.remove('annotations_trainval2017.zip')\n",
        "\n",
        "    def process_dataset(self):\n",
        "        coco = COCO(os.path.join(self.annotations_folder, 'annotations', 'instances_train2017.json'))\n",
        "\n",
        "        categories = coco.loadCats(coco.getCatIds())\n",
        "        category_map = {category['id']: category['name'] for category in categories}\n",
        "\n",
        "        # Initializing dictionaries to store images and counts per class\n",
        "        class_images = {object_class: [] for object_class in self.object_classes}\n",
        "        class_counts = {object_class: 0 for object_class in self.object_classes}\n",
        "\n",
        "        for image_id in coco.getImgIds():\n",
        "            image = coco.loadImgs(image_id)[0]\n",
        "            annotations = coco.loadAnns(coco.getAnnIds(imgIds=image['id']))\n",
        "\n",
        "            # Check if image contains desired object\n",
        "            object_class_present = False\n",
        "            for annotation in annotations:\n",
        "                category_id = annotation['category_id']\n",
        "                if category_map[category_id] in self.object_classes:\n",
        "                    object_class_present = True\n",
        "                    break\n",
        "\n",
        "            if not object_class_present:\n",
        "                continue\n",
        "\n",
        "            image_path = os.path.join(self.dataset_folder, 'train2017', image['file_name'])\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            for annotation in annotations:\n",
        "                category_id = annotation['category_id']\n",
        "                object_class = category_map[category_id]\n",
        "\n",
        "                if object_class not in self.object_classes:\n",
        "                    continue\n",
        "\n",
        "                # Checking if size of object is big enough\n",
        "                bbox = annotation['bbox']\n",
        "                object_size = max(bbox[2], bbox[3])\n",
        "                if object_size < self.min_object_size:\n",
        "                    continue\n",
        "\n",
        "                # Check if we have already collected enough images for this class\n",
        "                if class_counts[object_class] >= self.num_images_per_class:\n",
        "                    continue\n",
        "\n",
        "                # Croping and resizing image\n",
        "                x, y, w, h = [int(val) for val in bbox]\n",
        "                x1, y1, x2, y2 = x, y, x + w, y + h\n",
        "                crop_size = max(w, h)\n",
        "                center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "                x1 = center_x - crop_size // 2\n",
        "                y1 = center_y - crop_size // 2\n",
        "                x2 = x1 + crop_size\n",
        "                y2 = y1 + crop_size\n",
        "                crop = img[y1:y2, x1:x2]\n",
        "\n",
        "                if crop.size == 0:\n",
        "                    continue\n",
        "                resized_crop = cv2.resize(crop, (self.image_size, self.image_size))\n",
        "\n",
        "                # Compute hash of image and check if it has already been saved\n",
        "                with open(image_path, 'rb') as f:\n",
        "                    image_data = f.read()\n",
        "                image_hash = hashlib.sha256(image_data).hexdigest()\n",
        "                if image_hash not in class_images[object_class]:\n",
        "                    # Saving image to new path\n",
        "                    folder_path = os.path.join(self.resized_folder, object_class)\n",
        "                    if not os.path.exists(folder_path):\n",
        "                        os.makedirs(folder_path)\n",
        "                    image_path = os.path.join(folder_path, f'{image_hash}.jpg')\n",
        "                    cv2.imwrite(image_path, resized_crop)\n",
        "    \n",
        "                    class_images[object_class].append(image_hash)\n",
        "                    class_counts[object_class] += 1\n",
        "    \n",
        "                # exit loop after collecting images\n",
        "                if class_counts[object_class] >= self.num_images_per_class:\n",
        "                    break"
      ],
      "metadata": {
        "id": "eWsB1WgTy1eZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "image_size = 32\n",
        "num_images_per_class = 5000\n",
        "min_object_size = 16\n",
        "\n",
        "downloader = CocoDatasetDownloader(object_classes, num_images_per_class, min_object_size, image_size)\n",
        "downloader.download_dataset()\n",
        "downloader.process_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDvMSUcv0E9j",
        "outputId": "16eef517-6c29-4100-e2de-711e34295e9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading COCO dataset...\n",
            "Downloading COCO annotations...\n",
            "loading annotations into memory...\n",
            "Done (t=16.78s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLASSICAL ML APPROACH"
      ],
      "metadata": {
        "id": "ZQN-qXgGFFVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageClassifier:\n",
        "    def __init__(self, dataset_folder, object_classes, test_size=0.2, random_state=42, stratify=None):\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.object_classes = object_classes\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.stratify = stratify\n",
        "    \n",
        "    def load_data(self):\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for object_class in self.object_classes:\n",
        "            folder_path = os.path.join(self.dataset_folder, object_class)\n",
        "            image_paths = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
        "            for image_path in image_paths:\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is not None:\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
        "                    image = cv2.resize(image, (32, 32))\n",
        "                    X.append(image)\n",
        "                    y.append(object_class)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Spliting data into train and validation sets\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.test_size, random_state=self.random_state, stratify=self.stratify)\n",
        "\n",
        "        # Reshaping images to 2D arrays\n",
        "        nsamples, nx, ny, nrgb = X_train.shape\n",
        "        X_train = X_train.reshape((nsamples, nx * ny * nrgb))\n",
        "        nsamples, nx, ny, nrgb = X_val.shape\n",
        "        X_val = X_val.reshape((nsamples, nx * ny * nrgb))\n",
        "\n",
        "        self.X_train = X_train\n",
        "        self.X_val = X_val\n",
        "        self.y_train = y_train\n",
        "        self.y_val = y_val\n",
        "    \n",
        "    def train_and_evaluate(self, classifiers):\n",
        "        for clf_dict in classifiers:\n",
        "            clf_name = clf_dict['name']\n",
        "            clf = clf_dict['clf'](**clf_dict['params'])\n",
        "            clf.fit(self.X_train, self.y_train)\n",
        "            y_val_pred = clf.predict(self.X_val)\n",
        "            print(f\"{clf_name} Classification Report:\")\n",
        "            print(classification_report(self.y_val, y_val_pred))"
      ],
      "metadata": {
        "id": "iJCKd1tX4tOg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = '/content/coco_resized'\n",
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "classifiers = [{'name': 'Random Forest', 'clf': RandomForestClassifier, 'params': {'n_estimators': 200}}]\n",
        "\n",
        "ic = ImageClassifier(dataset_folder, object_classes)\n",
        "ic.load_data()\n",
        "ic.train_and_evaluate(classifiers)"
      ],
      "metadata": {
        "id": "njSqOn9C4u4S",
        "outputId": "cbf7190d-2aaf-4846-dc2a-d8f1bcab82bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     airplane       0.62      0.62      0.62       917\n",
            "         bowl       0.44      0.45      0.45       995\n",
            "          bus       0.46      0.57      0.51       963\n",
            "        clock       0.68      0.61      0.65      1013\n",
            "          cup       0.49      0.43      0.46      1026\n",
            "traffic light       0.63      0.75      0.69      1012\n",
            "     umbrella       0.51      0.40      0.45      1014\n",
            "\n",
            "     accuracy                           0.55      6940\n",
            "    macro avg       0.55      0.55      0.55      6940\n",
            " weighted avg       0.55      0.55      0.54      6940\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEEP LEARNING APPROACH WITH KERAS API\n"
      ],
      "metadata": {
        "id": "_EzaVSpy1F6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectClassifier:\n",
        "    def __init__(self, dataset_folder, object_classes):\n",
        "      # Initializing the ObjectClassifier class with the dataset folder and object classes\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.object_classes = object_classes\n",
        "        self.num_classes = len(self.object_classes)\n",
        "        self.label_map = {object_class: i for i, object_class in enumerate(self.object_classes)}\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "      # Loading and preprocessing the data\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for object_class in self.object_classes:\n",
        "            folder_path = os.path.join(self.dataset_folder, object_class)\n",
        "            image_paths = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
        "            for image_path in image_paths:\n",
        "                image = cv2.imread(image_path)\n",
        "                if image is not None:\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    image = cv2.resize(image, (32, 32))\n",
        "                    X.append(image)\n",
        "                    y.append(object_class)\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Splitting the data into training and validation sets and normalizing the pixel values\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        X_train = X_train / 255.0\n",
        "        X_val = X_val / 255.0\n",
        "\n",
        "        # Converting the labels to numerical values and one-hot encoding them\n",
        "        y_train_num = np.array([self.label_map[label] for label in y_train])\n",
        "        y_val_num = np.array([self.label_map[label] for label in y_val])\n",
        "\n",
        "        y_train_onehot = to_categorical(y_train_num)\n",
        "        y_val_onehot = to_categorical(y_val_num)\n",
        "\n",
        "        return X_train, X_val, y_train_onehot, y_val_onehot, y_val_num\n",
        "\n",
        "    def create_model(self):\n",
        "        # Creating a convolutional neural network model\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model  \n",
        "\n",
        "    def train_and_evaluate(self, num_epochs=100, batch_size=64, patience=100):\n",
        "      # Training and evaluating the model\n",
        "        X_train, X_val, y_train_onehot, y_val_onehot, y_val_num = self.load_and_preprocess_data()\n",
        "        model = self.create_model()\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss', mode='max', patience=patience)\n",
        "\n",
        "        history = model.fit(X_train, y_train_onehot, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val_onehot), callbacks=[early_stop])\n",
        "\n",
        "        y_pred_onehot = model.predict(X_val)\n",
        "        y_pred_num = np.argmax(y_pred_onehot, axis=1)\n",
        "\n",
        "        # Generating the classification report and printing it\n",
        "        report = classification_report(y_val_num, y_pred_num, target_names=self.object_classes)\n",
        "        print('\\nClassification Report:')\n",
        "        print(report)\n",
        "\n",
        "        accuracy = accuracy_score(y_val_num, y_pred_num)\n",
        "\n",
        "        # Ploting average of F1 score, accuracy, precision, and recall\n",
        "        report_dict = classification_report(y_val_num, y_pred_num, target_names=self.object_classes, output_dict=True)\n",
        "        averages = ['macro avg', 'weighted avg']\n",
        "        plot_data = {}\n",
        "\n",
        "        for avg in averages:\n",
        "            plot_data[avg] = [\n",
        "                report_dict[avg]['precision'],\n",
        "                report_dict[avg]['recall'],\n",
        "                report_dict[avg]['f1-score'],\n",
        "                accuracy\n",
        "            ]\n",
        "\n",
        "        plot_df = pd.DataFrame.from_dict(plot_data, orient='index', columns=['precision', 'recall', 'F1-score', 'accuracy'])\n",
        "        sns.set(style=\"whitegrid\")\n",
        "        sns.set_palette(\"pastel\")\n",
        "        ax = sns.barplot(data=plot_df,  palette=sns.color_palette(\"pastel\"), linewidth=1.5)\n",
        "        ax.set(ylabel='Metrics', xlabel='Average', title='Average of F1 score, accuracy, precision and recall')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "OMV6p7F_EfSJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_classes = ['traffic light', 'clock', 'cup', 'airplane', 'bus', 'umbrella', 'bowl']\n",
        "dataset_folder = '/content/coco_resized'\n",
        "\n",
        "classifier = ObjectClassifier(dataset_folder, object_classes)\n",
        "classifier.train_and_evaluate()"
      ],
      "metadata": {
        "id": "8ZnA7iZNEfWp",
        "outputId": "8ead34c4-10ce-4294-e7f7-e4ed35e9d723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "434/434 [==============================] - 22s 16ms/step - loss: 1.6468 - accuracy: 0.4464 - val_loss: 1.1736 - val_accuracy: 0.5821\n",
            "Epoch 2/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 1.0889 - accuracy: 0.6176 - val_loss: 0.9490 - val_accuracy: 0.6682\n",
            "Epoch 3/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.9301 - accuracy: 0.6728 - val_loss: 0.8375 - val_accuracy: 0.6976\n",
            "Epoch 4/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.8432 - accuracy: 0.7055 - val_loss: 0.8033 - val_accuracy: 0.7171\n",
            "Epoch 5/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.7794 - accuracy: 0.7286 - val_loss: 0.7691 - val_accuracy: 0.7269\n",
            "Epoch 6/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.7445 - accuracy: 0.7398 - val_loss: 0.7376 - val_accuracy: 0.7432\n",
            "Epoch 7/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.6823 - accuracy: 0.7636 - val_loss: 0.6692 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.6643 - accuracy: 0.7670 - val_loss: 0.6393 - val_accuracy: 0.7801\n",
            "Epoch 9/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.6213 - accuracy: 0.7797 - val_loss: 0.6252 - val_accuracy: 0.7865\n",
            "Epoch 10/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.5893 - accuracy: 0.7926 - val_loss: 0.6339 - val_accuracy: 0.7843\n",
            "Epoch 11/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.5711 - accuracy: 0.7972 - val_loss: 0.6483 - val_accuracy: 0.7782\n",
            "Epoch 12/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.5501 - accuracy: 0.8070 - val_loss: 0.6177 - val_accuracy: 0.7869\n",
            "Epoch 13/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.5262 - accuracy: 0.8155 - val_loss: 0.6168 - val_accuracy: 0.7879\n",
            "Epoch 14/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.5052 - accuracy: 0.8195 - val_loss: 0.6165 - val_accuracy: 0.7914\n",
            "Epoch 15/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.4944 - accuracy: 0.8227 - val_loss: 0.5973 - val_accuracy: 0.8007\n",
            "Epoch 16/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.4655 - accuracy: 0.8334 - val_loss: 0.6145 - val_accuracy: 0.7963\n",
            "Epoch 17/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.4437 - accuracy: 0.8412 - val_loss: 0.6101 - val_accuracy: 0.7997\n",
            "Epoch 18/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.4334 - accuracy: 0.8439 - val_loss: 0.5897 - val_accuracy: 0.8049\n",
            "Epoch 19/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.4130 - accuracy: 0.8534 - val_loss: 0.6191 - val_accuracy: 0.8068\n",
            "Epoch 20/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.4024 - accuracy: 0.8550 - val_loss: 0.6032 - val_accuracy: 0.8040\n",
            "Epoch 21/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.3921 - accuracy: 0.8595 - val_loss: 0.6073 - val_accuracy: 0.8065\n",
            "Epoch 22/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.3754 - accuracy: 0.8661 - val_loss: 0.6170 - val_accuracy: 0.7970\n",
            "Epoch 23/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.3669 - accuracy: 0.8688 - val_loss: 0.6298 - val_accuracy: 0.7999\n",
            "Epoch 24/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.3483 - accuracy: 0.8736 - val_loss: 0.5909 - val_accuracy: 0.8147\n",
            "Epoch 25/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.3448 - accuracy: 0.8761 - val_loss: 0.6551 - val_accuracy: 0.7968\n",
            "Epoch 26/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.3400 - accuracy: 0.8755 - val_loss: 0.6309 - val_accuracy: 0.8003\n",
            "Epoch 27/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.3238 - accuracy: 0.8855 - val_loss: 0.6405 - val_accuracy: 0.8073\n",
            "Epoch 28/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.3157 - accuracy: 0.8881 - val_loss: 0.6178 - val_accuracy: 0.8135\n",
            "Epoch 29/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.3067 - accuracy: 0.8911 - val_loss: 0.6259 - val_accuracy: 0.8075\n",
            "Epoch 30/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2967 - accuracy: 0.8913 - val_loss: 0.6546 - val_accuracy: 0.8059\n",
            "Epoch 31/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2923 - accuracy: 0.8916 - val_loss: 0.6400 - val_accuracy: 0.8107\n",
            "Epoch 32/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2839 - accuracy: 0.8975 - val_loss: 0.6666 - val_accuracy: 0.8029\n",
            "Epoch 33/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2782 - accuracy: 0.9006 - val_loss: 0.6694 - val_accuracy: 0.8049\n",
            "Epoch 34/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2695 - accuracy: 0.9041 - val_loss: 0.7077 - val_accuracy: 0.7918\n",
            "Epoch 35/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2731 - accuracy: 0.9017 - val_loss: 0.6059 - val_accuracy: 0.8151\n",
            "Epoch 36/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2660 - accuracy: 0.9034 - val_loss: 0.6591 - val_accuracy: 0.8105\n",
            "Epoch 37/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2505 - accuracy: 0.9091 - val_loss: 0.6646 - val_accuracy: 0.8117\n",
            "Epoch 38/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2515 - accuracy: 0.9107 - val_loss: 0.6684 - val_accuracy: 0.8153\n",
            "Epoch 39/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2466 - accuracy: 0.9093 - val_loss: 0.6894 - val_accuracy: 0.8043\n",
            "Epoch 40/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2422 - accuracy: 0.9121 - val_loss: 0.6493 - val_accuracy: 0.8143\n",
            "Epoch 41/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2452 - accuracy: 0.9114 - val_loss: 0.6727 - val_accuracy: 0.8117\n",
            "Epoch 42/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2283 - accuracy: 0.9180 - val_loss: 0.6758 - val_accuracy: 0.8125\n",
            "Epoch 43/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2263 - accuracy: 0.9187 - val_loss: 0.6676 - val_accuracy: 0.8128\n",
            "Epoch 44/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2312 - accuracy: 0.9179 - val_loss: 0.6807 - val_accuracy: 0.8095\n",
            "Epoch 45/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2248 - accuracy: 0.9189 - val_loss: 0.6668 - val_accuracy: 0.8127\n",
            "Epoch 46/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2136 - accuracy: 0.9232 - val_loss: 0.6554 - val_accuracy: 0.8115\n",
            "Epoch 47/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2145 - accuracy: 0.9230 - val_loss: 0.7152 - val_accuracy: 0.8133\n",
            "Epoch 48/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2118 - accuracy: 0.9253 - val_loss: 0.6921 - val_accuracy: 0.8137\n",
            "Epoch 49/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2003 - accuracy: 0.9262 - val_loss: 0.7008 - val_accuracy: 0.8097\n",
            "Epoch 50/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.2036 - accuracy: 0.9275 - val_loss: 0.7206 - val_accuracy: 0.8157\n",
            "Epoch 51/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.2031 - accuracy: 0.9287 - val_loss: 0.7170 - val_accuracy: 0.8099\n",
            "Epoch 52/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1972 - accuracy: 0.9295 - val_loss: 0.7177 - val_accuracy: 0.8148\n",
            "Epoch 53/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1988 - accuracy: 0.9296 - val_loss: 0.7157 - val_accuracy: 0.8125\n",
            "Epoch 54/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1877 - accuracy: 0.9324 - val_loss: 0.7170 - val_accuracy: 0.8135\n",
            "Epoch 55/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1911 - accuracy: 0.9317 - val_loss: 0.7280 - val_accuracy: 0.8163\n",
            "Epoch 56/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1866 - accuracy: 0.9349 - val_loss: 0.7246 - val_accuracy: 0.8092\n",
            "Epoch 57/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1833 - accuracy: 0.9331 - val_loss: 0.7458 - val_accuracy: 0.8134\n",
            "Epoch 58/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1777 - accuracy: 0.9364 - val_loss: 0.7288 - val_accuracy: 0.8148\n",
            "Epoch 59/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1796 - accuracy: 0.9367 - val_loss: 0.7484 - val_accuracy: 0.8128\n",
            "Epoch 60/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1773 - accuracy: 0.9351 - val_loss: 0.7330 - val_accuracy: 0.8115\n",
            "Epoch 61/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.1781 - accuracy: 0.9367 - val_loss: 0.7282 - val_accuracy: 0.8078\n",
            "Epoch 62/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1733 - accuracy: 0.9386 - val_loss: 0.7166 - val_accuracy: 0.8186\n",
            "Epoch 63/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1727 - accuracy: 0.9388 - val_loss: 0.7040 - val_accuracy: 0.8192\n",
            "Epoch 64/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1687 - accuracy: 0.9399 - val_loss: 0.7558 - val_accuracy: 0.8176\n",
            "Epoch 65/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1683 - accuracy: 0.9400 - val_loss: 0.7379 - val_accuracy: 0.8118\n",
            "Epoch 66/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1615 - accuracy: 0.9422 - val_loss: 0.7490 - val_accuracy: 0.8186\n",
            "Epoch 67/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1678 - accuracy: 0.9393 - val_loss: 0.7593 - val_accuracy: 0.8084\n",
            "Epoch 68/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1637 - accuracy: 0.9411 - val_loss: 0.7202 - val_accuracy: 0.8179\n",
            "Epoch 69/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1598 - accuracy: 0.9439 - val_loss: 0.7672 - val_accuracy: 0.8137\n",
            "Epoch 70/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.1578 - accuracy: 0.9444 - val_loss: 0.7451 - val_accuracy: 0.8156\n",
            "Epoch 71/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1565 - accuracy: 0.9441 - val_loss: 0.7301 - val_accuracy: 0.8161\n",
            "Epoch 72/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1573 - accuracy: 0.9442 - val_loss: 0.7608 - val_accuracy: 0.8104\n",
            "Epoch 73/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1571 - accuracy: 0.9443 - val_loss: 0.7731 - val_accuracy: 0.8150\n",
            "Epoch 74/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1551 - accuracy: 0.9456 - val_loss: 0.7401 - val_accuracy: 0.8231\n",
            "Epoch 75/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1552 - accuracy: 0.9442 - val_loss: 0.7613 - val_accuracy: 0.8140\n",
            "Epoch 76/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1454 - accuracy: 0.9486 - val_loss: 0.7851 - val_accuracy: 0.8193\n",
            "Epoch 77/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1516 - accuracy: 0.9465 - val_loss: 0.7473 - val_accuracy: 0.8117\n",
            "Epoch 78/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.1436 - accuracy: 0.9501 - val_loss: 0.7536 - val_accuracy: 0.8131\n",
            "Epoch 79/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1443 - accuracy: 0.9494 - val_loss: 0.7801 - val_accuracy: 0.8182\n",
            "Epoch 80/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1378 - accuracy: 0.9513 - val_loss: 0.7899 - val_accuracy: 0.8131\n",
            "Epoch 81/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1384 - accuracy: 0.9500 - val_loss: 0.7774 - val_accuracy: 0.8182\n",
            "Epoch 82/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1440 - accuracy: 0.9499 - val_loss: 0.7679 - val_accuracy: 0.8137\n",
            "Epoch 83/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1460 - accuracy: 0.9486 - val_loss: 0.8124 - val_accuracy: 0.8092\n",
            "Epoch 84/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1383 - accuracy: 0.9508 - val_loss: 0.7773 - val_accuracy: 0.8117\n",
            "Epoch 85/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1352 - accuracy: 0.9506 - val_loss: 0.8191 - val_accuracy: 0.8146\n",
            "Epoch 86/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1370 - accuracy: 0.9528 - val_loss: 0.8023 - val_accuracy: 0.8121\n",
            "Epoch 87/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1385 - accuracy: 0.9504 - val_loss: 0.8024 - val_accuracy: 0.8101\n",
            "Epoch 88/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1397 - accuracy: 0.9502 - val_loss: 0.7791 - val_accuracy: 0.8186\n",
            "Epoch 89/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1299 - accuracy: 0.9558 - val_loss: 0.7812 - val_accuracy: 0.8170\n",
            "Epoch 90/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1229 - accuracy: 0.9565 - val_loss: 0.8246 - val_accuracy: 0.8154\n",
            "Epoch 91/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1309 - accuracy: 0.9549 - val_loss: 0.8123 - val_accuracy: 0.8091\n",
            "Epoch 92/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1335 - accuracy: 0.9540 - val_loss: 0.8278 - val_accuracy: 0.8177\n",
            "Epoch 93/100\n",
            "434/434 [==============================] - 6s 15ms/step - loss: 0.1318 - accuracy: 0.9545 - val_loss: 0.7795 - val_accuracy: 0.8182\n",
            "Epoch 94/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1289 - accuracy: 0.9545 - val_loss: 0.7868 - val_accuracy: 0.8206\n",
            "Epoch 95/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1251 - accuracy: 0.9558 - val_loss: 0.8460 - val_accuracy: 0.8125\n",
            "Epoch 96/100\n",
            "434/434 [==============================] - 6s 13ms/step - loss: 0.1329 - accuracy: 0.9545 - val_loss: 0.7545 - val_accuracy: 0.8199\n",
            "Epoch 97/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1232 - accuracy: 0.9571 - val_loss: 0.8330 - val_accuracy: 0.8154\n",
            "Epoch 98/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1276 - accuracy: 0.9541 - val_loss: 0.7771 - val_accuracy: 0.8177\n",
            "Epoch 99/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1214 - accuracy: 0.9560 - val_loss: 0.8532 - val_accuracy: 0.8085\n",
            "Epoch 100/100\n",
            "434/434 [==============================] - 6s 14ms/step - loss: 0.1269 - accuracy: 0.9561 - val_loss: 0.7750 - val_accuracy: 0.8160\n",
            "217/217 [==============================] - 1s 3ms/step\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "traffic light       0.89      0.90      0.89      1000\n",
            "        clock       0.89      0.88      0.88      1000\n",
            "          cup       0.74      0.78      0.76      1000\n",
            "     airplane       0.78      0.85      0.81       940\n",
            "          bus       0.80      0.82      0.81      1000\n",
            "     umbrella       0.86      0.74      0.79      1000\n",
            "         bowl       0.76      0.75      0.76      1000\n",
            "\n",
            "     accuracy                           0.82      6940\n",
            "    macro avg       0.82      0.82      0.82      6940\n",
            " weighted avg       0.82      0.82      0.82      6940\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4kCLwF1RUObY"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}